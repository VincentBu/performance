#!meta

{"kernelInfo":{"defaultKernelName":"csharp","items":[{"aliases":[],"name":"csharp"}]}}

#!csharp

#!import BenchmarkAnalysis.dib

#!csharp

List<(SeriesInfo<TData>, List<KeyValuePair<string, TData>>)> GetDataInternal<TData>(ChartType<TData> chartType,
    DataManager dataManager, List<Metric<TData>> metrics,
    Filter runFilter = null, Filter benchmarkFilter = null, IntFilter iterationFilter = null,
    ConfigIterationFilter configIterationFilter = null, Func<TData, bool> dataFilter = null, XArrangement xArrangement = null,
    NameSimplifier configNameSimplifier = null, bool includeRunName = false, Filter configFilter = null, bool debug = false)
{
    runFilter = runFilter ?? Filter.All;
    benchmarkFilter = benchmarkFilter ?? Filter.All;
    iterationFilter = iterationFilter ?? IntFilter.All;
    configFilter = configFilter ?? Filter.All;
    // configIterationFilter is not set to an empty dictionary as that would exclude everything
    dataFilter = dataFilter ?? (data => true);
    var benchmarkMap = chartType.DefaultBenchmarkMap;
    var xMetric = chartType.DefaultXMetric;
    xArrangement = xArrangement ?? XArrangements.Default;

    List<(SeriesInfo<TData>, List<KeyValuePair<string, TData>>)> dataSources = new();

    if (metrics.Count == 0)
    {
        Console.WriteLine("No metrics");
        return dataSources;
    }

    List<string> configs = dataManager.GetConfigs(runFilter: runFilter, configFilter: configFilter).Select(tuple => tuple.config).Distinct().ToList();
    if (configs.Count == 0)
    {
        Console.WriteLine("No configs afer filtering");
        return dataSources;
    }

    if (debug) Console.WriteLine("Simplify config names");
    Dictionary<string, string> configDisplayNames = null;
    string configPrefix = null;
    if (configNameSimplifier != null)
    {
        (configPrefix, configDisplayNames) = configNameSimplifier.Simplify(configs);
    }
    
    if (debug) Console.WriteLine("Prepare units");

    Dictionary<string, List<string>> benchmarkGroups = new();
    HashSet<string> benchmarkSet = new();
    foreach ((string run, string config, string benchmark) in
        dataManager.GetBenchmarks(runFilter: runFilter, configFilter: configFilter, benchmarkFilter: benchmarkFilter, iterationFilter: iterationFilter,
            configIterationFilter: configIterationFilter))
    {
        if (!benchmarkSet.Add(benchmark)) continue;

        string benchmarkGroup = (benchmarkMap != null) ? benchmarkMap(benchmark) : benchmark;
        benchmarkGroups.GetOrAdd(benchmarkGroup, new());
        benchmarkGroups[benchmarkGroup].Add(benchmark);
    }

    foreach (var (benchmarkGroup, benchmarkList) in benchmarkGroups)
    {
        if (debug) Console.WriteLine("Initialize colors");

        {
            string xlabel = xArrangement.GetNewTitle(xMetric.Title);

            string titlePrefix = chartType.GetChartTitle();
            List<string> titleParts = new();
            if (!string.IsNullOrWhiteSpace(benchmarkGroup)) titleParts.Add(benchmarkGroup);
            if (metrics.Count == 1) titleParts.Add(metrics[0].Title);
            if (configPrefix != null) titleParts.Add(configPrefix);
            else if (configs.Count == 1) titleParts.Add(configDisplayNames?.GetValueOrDefault(configs[0]) ?? configs[0]);
            string titleWithoutPrefix = string.Join(" / ", titleParts);
            string title = string.Join(" / ", titleParts.Prepend(titlePrefix));

            List<(XValue x, double? y)> firstDataPreSorted = null;
            double firstDataMin = 0;
            HashSet<XValue> firstDataSet = new();

            foreach (SeriesInfo<TData> info in
                chartType.GetSeries(dataManager, metrics, runFilter: runFilter, configFilter: configFilter, benchmarkFilter: benchmarkFilter,
                    iterationFilter: iterationFilter, configIterationFilter: configIterationFilter, benchmarkList: benchmarkList))
            {
                string colorFamilyKey = chartType.GetColorFamilyKey(info, multipleMetrics: metrics.Count > 1, includeRunName: includeRunName, multipleConfigs: configs.Count > 1,
                    configDisplayNames: configDisplayNames, multipleBenchmarks: benchmarkList.Count > 1);
                string seriesTitle = chartType.GetSeriesTitle(info, colorFamilyKey, metrics.Count > 1);
                if (debug) Console.Write($"series title: {seriesTitle}, ");

                List<KeyValuePair<string, TData>> dataSource;
                try { dataSource = chartType.GetDataSource(info, benchmarkFilter: benchmarkFilter, iterationFilter: iterationFilter,
                    configIterationFilter: configIterationFilter, dataFilter: dataFilter); }
                catch (Exception e) { Console.WriteLine($"Exception {e} processing data source for {title} / {seriesTitle}"); dataSource = null; }
                if (dataSource == null)
                {
                    Console.WriteLine($"No data for {titleWithoutPrefix} / {seriesTitle}");
                    continue;
                }

                else
                {
                    dataSources.Add((info, dataSource));
                }
            }
        }
    }

    return dataSources;
}

#!csharp

Dictionary<string, Dictionary<string, Dictionary<string, double?>>> GetBenchmarkData(DataManager dataManager, List<Metric<BenchmarkData>> metrics,
    Filter runFilter = null, Filter configFilter = null, Filter benchmarkFilter = null, IntFilter iterationFilter = null,
    ConfigIterationFilter configIterationFilter = null, Func<BenchmarkData, bool> dataFilter = null,
    Func<string, string> benchmarkMap = null, XArrangement xArrangement = null,
    NameSimplifier configNameSimplifier = null, bool includeRunName = false,
    bool display = true, bool debug = false)
{
    BenchmarksChartType chartType = new BenchmarksChartType();
    var xMetric = chartType.DefaultXMetric; 
    var result = GetDataInternal(chartType: chartType,
        dataManager: dataManager, metrics: metrics,
        runFilter: runFilter, benchmarkFilter: benchmarkFilter, iterationFilter: iterationFilter, configFilter: configFilter,
        configIterationFilter: configIterationFilter, dataFilter: dataFilter,
        xArrangement: xArrangement,
        configNameSimplifier: configNameSimplifier, includeRunName: includeRunName, debug: debug);

    Dictionary<string, Dictionary<string, Dictionary<string, double?>>> data = new();
    //foreach (var r in result)
    for (int i = 0; i < result.Count; i++)
    {
        var r = result[i];

        List<(XValue, double?)> benchmarkMetricData = null;
        try { benchmarkMetricData = r.Item2.Select(b => (x: xMetric.DoExtract((b.Key, b.Value)), y: r.Item1.Metric.DoExtract(b.Value, i))).ToList(); }
        catch { Console.WriteLine($"Exception processing data items"); benchmarkMetricData = null; } 

        // Config -> Metric -> (Benchmark, Data).
        if (benchmarkMetricData != null)
        {
            if (!data.TryGetValue(r.Item1.Config, out var configData))
            {
                data[r.Item1.Config] = configData = new();
            }

            if (!configData.TryGetValue(r.Item1.Metric.Title, out var runData))
            {
                configData[r.Item1.Metric.Title] = runData = new();
            }

            foreach (var benchmark in benchmarkMetricData)
            {
                runData[benchmark.Item1.GetName()] = benchmark.Item2;
            }
        }

        else
        {
            // TODO: Log.
        }
    }

    return data;
}

#!csharp

public enum ExpectationDirection
{
    Unknown,
    Increase,
    Decrease,
    NoChange,
}

#!csharp

public record BenchmarkRegressionInfo(string benchmarkName, string metricInfo, double regressionPercentage);

static List<BenchmarkRegressionInfo> GetBenchmarkRegressions (
    this Dictionary<string, Dictionary<string, Dictionary<string, double?>>> data, string baselineConfig, string comparandConfig, double tolerancePercentage = 5, bool debug = false,
    Dictionary<Metric<BenchmarkData>, double> metricToleranceOverridePercentage = null, Dictionary<Metric<BenchmarkData>, ExpectationDirection> expectationDirections = null)
{
    metricToleranceOverridePercentage ??= new();
    expectationDirections ??= new();
    Dictionary<string, double> simplifiedMetricToleranceOverridePercentage = new();
    foreach (var metricToleranceOverride in metricToleranceOverridePercentage)
    {
        simplifiedMetricToleranceOverridePercentage[metricToleranceOverride.Key.Title] = metricToleranceOverride.Value;
    }

    Dictionary<string, ExpectationDirection> simplifiedExpectationDirections = new();
    foreach (var expectationDirection in expectationDirections)
    {
        simplifiedExpectationDirections[expectationDirection.Key.Title] = expectationDirection.Value;
    }

    List<BenchmarkRegressionInfo> regressions = new();
    double? percentDifference (double? baseline, double? comparand) => (comparand - baseline) / baseline * 100;

    // Preconditition Checks.
    if (!data.ContainsKey(baselineConfig) || !data.ContainsKey(comparandConfig))
    {
        if (debug)
        {
            Console.WriteLine($"Either {baselineConfig} or {comparandConfig} is not present in the data.");
        }

        return regressions;
    }

    // Metric -> BenchmarkData.
    var baselineData = data[baselineConfig];
    var comparandData = data[comparandConfig];

    foreach (var metricToBenchmarkData in baselineData)
    {
        string metric = metricToBenchmarkData.Key;
        if (comparandData.TryGetValue(metric, out var comparandSeries))
        {
            foreach (var b in metricToBenchmarkData.Value)
            {
                string benchmark = b.Key;
                if (!comparandSeries.TryGetValue(b.Key, out var c))
                {
                    if (debug)
                    {
                        Console.WriteLine($"Skipping comparison for {metricToBenchmarkData.Key} as one of the values is null.");
                    }
                    continue;
                }

                var diff = percentDifference(b.Value, c.Value);
                if (double.IsNaN(diff.GetValueOrDefault()) || double.IsInfinity(diff.GetValueOrDefault()))
                {
                    if (debug)
                    {
                        Console.WriteLine($"Obtained illegal diff for {b.Key} x {metricToBenchmarkData.Key} - Omitting");
                    }
                }

                void AddRegression (string benchmarkName, string metricInfo, double regressionPercentage, double threshold, ExpectationDirection direction)
                {
                    switch (direction)
                    {
                        // If the expectation is that the metric should increase and if we observe a decrease beyond the threshold, we add it to regressions. 
                        case ExpectationDirection.Increase:
                            if (regressionPercentage < 0 && (Math.Abs(regressionPercentage) > Math.Abs(tolerancePercentage)))
                            {
                                regressions.Add(new BenchmarkRegressionInfo(benchmarkName, metricInfo, regressionPercentage));
                            }
                            break;
                        // If the expectation is that the metric should decrease and if we observe an increase beyond the threshold, we add it to regressions. 
                        case ExpectationDirection.Decrease:
                            if (regressionPercentage > 0 && (Math.Abs(regressionPercentage) > Math.Abs(tolerancePercentage)))
                            {
                                regressions.Add(new BenchmarkRegressionInfo(benchmarkName, metricInfo, regressionPercentage));
                            }
                            break;
                        // Otherwise, we add the regression if the percentage difference is beyond the threshold.
                        default:
                            if (Math.Abs(diff.GetValueOrDefault()) > Math.Abs(tolerancePercentage))
                            {
                                regressions.Add(new BenchmarkRegressionInfo(benchmarkName, metricInfo, regressionPercentage));
                            }
                            break;
                    }
                }

                // If the override exists, use it.
                //if (simplifiedMetricToleranceOverridePercentage.TryGetValue(metricToBenchmarkData.Key, out var overrideValue))
                bool overrideExists = simplifiedMetricToleranceOverridePercentage.TryGetValue(metricToBenchmarkData.Key, out var overrideValue);
                overrideValue = overrideExists ? simplifiedMetricToleranceOverridePercentage[metricToBenchmarkData.Key] : tolerancePercentage;
                AddRegression(b.Key, metricToBenchmarkData.Key, Math.Round(diff.GetValueOrDefault(), 2), overrideValue, simplifiedExpectationDirections.GetValueOrDefault(metricToBenchmarkData.Key));
            }
        }

        else
        {
            if (debug)
            {
                Console.WriteLine($"Metric missing in comparand config: {metricToBenchmarkData.Key} -- skipping");
            }
        }
    }

    return regressions;
}

#!csharp

public static (int, double?) FindMostDifferentDataPoint(List<double?> data, double volatilityThreshold = 5)
{
    double? Abs(double? x) => x < 0 ? -x : x;
    double? volatility = (data.Max() - data.Min()) / data.Min() * 100;

    if (volatility > volatilityThreshold)
    {
        double? mostDifferentDataPoint = data.OrderByDescending(x => Abs(x.Value - data.Average(d => d))).FirstOrDefault();
        for (int i = 0; i < data.Count; i++)
        {
            if (data[i] == mostDifferentDataPoint)
            {
                return (i, volatility);
            }
        }

    }

    return (-1, volatility);
}

#!csharp

void DisplayBenchmarkSummaryWithRegressions(DataManager dm, string baselineConfigName, string comparandConfigName, List<Metric<BenchmarkData>> metrics = null, double benchmarkRegressionThreshold = 5.0,
    Filter runFilter = null, Filter configFilter = null, Filter benchmarkFilter = null, IntFilter iterationFilter = null,
    ConfigIterationFilter configIterationFilter = null, Func<BenchmarkData, bool> dataFilter = null,
    Func<string, string> benchmarkMap = null, XArrangement xArrangement = null,
    NameSimplifier configNameSimplifier = null, bool includeRunName = false,
    bool display = true, bool debug = false, Dictionary<Metric<BenchmarkData>, double> metricToleranceOverridePercentage = null, int benchmarksToDisplay = 5,
    Dictionary<Metric<BenchmarkData>, ExpectationDirection> expectationDirections = null)
{
    metrics ??= ML(Metrics.B.MaxMaxHeapSize, Metrics.B.MaxHeapSizeCVPerc, Metrics.B.AverageP50Latency, Metrics.B.P50LatencyCVPerc, Metrics.B.AverageRequestPerMSec, Metrics.B.RequestPerMSecCVPerc);
    var benchmarkData = 
        GetBenchmarkData(dm, metrics, runFilter, configFilter, benchmarkFilter, iterationFilter, configIterationFilter, dataFilter, benchmarkMap, xArrangement, configNameSimplifier, includeRunName, display, debug);
    int totalBenchmarks = benchmarkData.First().Value.First().Value.Count;
    List<BenchmarkRegressionInfo> regressions = 
        benchmarkData.GetBenchmarkRegressions(baselineConfig: baselineConfigName, comparandConfig: comparandConfigName, tolerancePercentage: benchmarkRegressionThreshold, debug: debug, metricToleranceOverridePercentage: metricToleranceOverridePercentage, expectationDirections: expectationDirections);

    HashSet<string> filteredBenchmarks = new();
    HashSet<string> filteredMetricSet = new();

    foreach (var regression in regressions)
    {
        filteredBenchmarks.Add(regression.benchmarkName);
        filteredMetricSet.Add(regression.metricInfo);
    }

    // Map metric string to Metric<Benchmark> list.
    metrics = metrics.Where(m => filteredMetricSet.Contains(m.Title)).ToList();
    StringBuilder sb = new();
    sb.AppendLine($"## Out of {totalBenchmarks} benchmarks, {filteredBenchmarks.Count} have regressed.");

    // TODO: Show the thresholds used.

    foreach (var regression in regressions.OrderByDescending(r => r.regressionPercentage).Take(benchmarksToDisplay))
    {
        sb.AppendLine($"   - __{regression.benchmarkName}__ has regressed by {regression.regressionPercentage}% for {regression.metricInfo}");
    }
    
    sb.ToString().DisplayAs("text/markdown");

    if (display)
    {
        TableBenchmarks(dm, metrics, configCompareInfo: new ConfigCompareInfo(baselineConfigName, comparandConfigName), 
        runFilter: runFilter, benchmarkFilter: new Filter(filteredBenchmarks), iterationFilter: iterationFilter, 
        benchmarkMap: benchmarkMap, xArrangement: xArrangement, configNameSimplifier: configNameSimplifier, includeRunName: includeRunName, display: display, debug: debug);
    }
}

#!csharp

public class Cluster
{
    public List<double> DataPoints { get; set; } = new List<double>();
    public double Average => DataPoints.Count > 0 ? DataPoints.Average() : 0;
    public double Count => DataPoints.Count;
    public static double CalculateMedian(List<double> sortedData)
    {
        sortedData.Sort();
        int mid = sortedData.Count / 2;
        return sortedData.Count % 2 != 0 ? sortedData[mid] : (sortedData[mid - 1] + sortedData[mid]) / 2.0;
    }
}

public sealed class ClusteringResult
{
    public ClusteringResult(List<Cluster> clusters, List<double> anomalies)
    {
        Clusters = clusters;
        Anomalies = anomalies;
    }

    public List<Cluster> Clusters { get; } 
    public List<double> Anomalies { get; }
    // Single element clusters and anomalies detected indicate an anomaly in the data.
    public bool HasAnomaly => Anomalies.Count > 0 || Clusters.Any(c => c.Count == 1);
}

public class Clustering
{
    public static ClusteringResult KMeansClusteringWithAnomalyDetection(List<double?> data, int? numberOfClusters = null, double? anomalyThreshold = null, int maxIterations = 100)
    {
        var dataPoints = data.Where(d => d.HasValue).Select(d => (double)d).ToList();
        numberOfClusters ??= data.Count() / 2 + 1;
        int k = numberOfClusters.Value;
        if (dataPoints.Count == 0 || k <= 0 || k > dataPoints.Count) throw new ArgumentException("Invalid data or cluster count");

        anomalyThreshold ??= dataPoints.Average() + dataPoints.StandardDeviation() * 2; // Using mean + 2σ for anomaly detection 

        // Step 1: Initialize centroids randomly
        Random random = new Random();
        var centroids = dataPoints.OrderBy(_ => random.Next()).Take(k).ToList();
        var clusters = new List<Cluster>();
        var anomalies = new List<double>();

        for (int iteration = 0; iteration < maxIterations; iteration++)
        {
            // Step 2: Assign points to the nearest centroid
            clusters = Enumerable.Range(0, k).Select(_ => new Cluster()).ToList();

            foreach (var point in dataPoints)
            {
                var nearestCentroid = centroids
                    .Select((c, index) => new { Index = index, Distance = Math.Abs(point - c) })
                    .OrderBy(x => x.Distance)
                    .First();

                if (nearestCentroid.Distance > anomalyThreshold)
                {
                    // Mark as anomaly if the distance is beyond the threshold
                    anomalies.Add(point);
                }
                else
                {
                    // Otherwise, assign to the nearest cluster
                    clusters[nearestCentroid.Index].DataPoints.Add(point);
                }
            }

            // Step 3: Recalculate centroids
            var newCentroids = clusters.Select(cluster => cluster.DataPoints.Count > 0 ? cluster.Average : 0).ToList();

            // Check for convergence (if centroids haven't changed, break the loop)
            if (centroids.SequenceEqual(newCentroids))
                break;

            centroids = newCentroids;
        }

        return new ClusteringResult(clusters, anomalies);
    }

    public static void DisplayClustersAndAnomaliesTextually(ClusteringResult result)
    {
        for (int i = 0; i < result.Clusters.Count; i++)
        {
            Console.WriteLine($"Cluster {i + 1}:");
            Console.WriteLine($" - Data Points: {string.Join(", ", result.Clusters[i].DataPoints)}");
            Console.WriteLine($" - Count: {result.Clusters[i].Count}");
            Console.WriteLine($" - Average: {result.Clusters[i].Average:F2}");
            Console.WriteLine();
        }

        Console.WriteLine("Anomalies:");
        if (result.Anomalies.Count > 0)
            Console.WriteLine($" - {string.Join(", ", result.Anomalies)}");
        else
            Console.WriteLine(" - None");
    }
}

#!csharp

public record IterationDetails(string configName, int iterationNumber, Dictionary<string, double?> data);

void DisplayIterationData(DataManager dataManager, string baselineConfigName, string comparandConfigName = null, List<Metric<IterationData>> metrics = null,
    Filter runFilter = null, Filter benchmarkFilter = null, IntFilter iterationFilter = null,
    ConfigIterationFilter configIterationFilter = null, Func<IterationData, bool> dataFilter = null,
    Func<string, string> benchmarkMap = null, XArrangement xArrangement = null,
    NameSimplifier configNameSimplifier = null, bool includeRunName = false,
    bool display = true, bool debug = false, double volatilityThreshold = 5, Dictionary<Metric<IterationData>, double> metricVolOverridePercentage = null, Aggregation aggregation = null
    , int iterationsToDisplay = 5)
{
    comparandConfigName ??= "";
    metricVolOverridePercentage ??= new();
    aggregation ??= Aggregation.Average;
    IterationsChartType chartType = new IterationsChartType();
    var xMetric = chartType.DefaultXMetric; 
    metrics ??= ML(Metrics.I.Gen0Count, Metrics.I.Gen0MeanPauseMSec, 
                   Metrics.I.Gen1Count, Metrics.I.Gen1MeanPauseMSec, 
                   Metrics.I.BlockingGCCount, Metrics.I.BlockingGCMeanPauseTime, 
                   Metrics.I.BGCCount, Metrics.I.BGCMeanPauseTime, 
                   Metrics.I.TotalAllocationsMB, Metrics.I.MeanAllocPerGC,
                   Metrics.I.PercentPauseTimeInGC, Metrics.I.MeanHeapSizeBefore, Metrics.I.MeanHeapSizeBeforeMB, 
                   Metrics.I.MaxWorkingSetMB, Metrics.I.RequestsPerMSec, Metrics.I.MeanLatencyMS, Metrics.I.NumberOfHeapCountSwitches, Metrics.I.TotalGCCount);
    var result = GetDataInternal(chartType: chartType,
        dataManager: dataManager, metrics: metrics,
        runFilter: runFilter, benchmarkFilter: benchmarkFilter, iterationFilter: iterationFilter, configFilter: new Filter( baselineConfigName, comparandConfigName ),
        configIterationFilter: configIterationFilter, dataFilter: dataFilter,
        xArrangement: xArrangement,
        configNameSimplifier: configNameSimplifier, includeRunName: includeRunName, debug: debug);

    string GetConfigFromKey(string key) => key.Split('|')[0];
    string GetBenchmarkFromKey(string key) => key.Split('|')[1];
    string GetMetricFromKey(string key) => key.Split('|')[2];
    string CreateKey(string configName, string benchmarkName, string metricName) => $"{configName}|{benchmarkName}|{metricName}";

    // Benchmark -> (Key -> IterationDetails)
    Dictionary<string, Dictionary<string, IterationDetails>> iterationDetails = new();
    Dictionary<string, List<double?>> iterationData = new();

    List<(string key, int idx, double? anomolousVal, IEnumerable<double?> allVals, double volatility)> allAnomalies = new();

    // Config -> Benchmark -> (Iteration -> Data).
    // For a benchmark and config -> Intravol of the iterations.

    // Display Table that's keyed on benchmarks.
    foreach (var kvp in result)
    {
        foreach (var benchmark in kvp.Item2)
        {
            if (!iterationDetails.TryGetValue(benchmark.Key, out var d))
            {
                iterationDetails[benchmark.Key] = d = new();
            }

            int? iterationNumber = kvp.Item1.Iteration.GetValueOrDefault();
            // At the iteration level, what are the details.
            string key = $"{kvp.Item1.Config}_{benchmark.Key}_{iterationNumber}";
            if (!d.TryGetValue(key, out var data))
            {
                Dictionary<string, double?> metricData = new();
                foreach (var metric in metrics)
                {
                    metricData[metric.Title] = metric.DoExtract(benchmark.Value);
                    string iterationKey = CreateKey(kvp.Item1.Config, benchmark.Key, metric.Title);

                    if (!iterationData.TryGetValue(iterationKey, out var iData))
                    {
                        iterationData[iterationKey] = iData = new();
                    }

                    iData.Add(metricData[metric.Title]);
                }

                d[key] = new IterationDetails(configName: kvp.Item1.Config, iterationNumber: iterationNumber.GetValueOrDefault(), data: metricData);
            }
        }
    }

    StringBuilder table = new();
    foreach (var benchmark in iterationDetails)
    {
        table.AppendLine($"## {benchmark.Key}");
        table.AppendLine();
        table.AppendLine("| Config | Iteration | " + string.Join(" | ", metrics.Select(m => m.Title)) + " |");
        table.AppendLine("| ------ | --------- | " + string.Join(" | ", metrics.Select(m => "------")) + " |");
        foreach (var iteration in benchmark.Value)
        {
            table.AppendLine($"| {iteration.Value.configName} | {iteration.Value.iterationNumber} | " + string.Join(" | ", iteration.Value.data.Select(d => d.Value?.ToString("F2") ?? "N/A")) + " |");
        }

        // Get Config and Benchmark Data for Metrics.
        List<Metric<BenchmarkData>> benchmarkMetrics = metrics.Select(m => Metrics.Promote(m, aggregation)).ToList();
        var baselineData = GetBenchmarkData(dataManager: dataManager, metrics: benchmarkMetrics, configFilter: new Filter(baselineConfigName),  benchmarkFilter: new Filter(benchmark.Key), iterationFilter: iterationFilter, 
        configIterationFilter: configIterationFilter, benchmarkMap: benchmarkMap, xArrangement: xArrangement, configNameSimplifier: configNameSimplifier, includeRunName: includeRunName, display: display, debug: debug);
        IEnumerable<double?> baselineAggregatedMetrics = baselineData.SelectMany(d => d.Value.Values).SelectMany(d => d.Values);
        // Add baseline agg.
        table.AppendLine($"| {baselineConfigName} | {aggregation.Title} | " + string.Join(" | ", baselineAggregatedMetrics.Select(d => d.GetValueOrDefault().ToString("N2"))) + " |");

        // Add comparand agg.
        var comparandData = GetBenchmarkData(dataManager: dataManager, metrics: benchmarkMetrics, configFilter: new Filter(comparandConfigName),  benchmarkFilter: new Filter(benchmark.Key), iterationFilter: iterationFilter, 
        configIterationFilter: configIterationFilter, benchmarkMap: benchmarkMap, xArrangement: xArrangement, configNameSimplifier: configNameSimplifier, includeRunName: includeRunName, display: display, debug: debug);
        IEnumerable<double?> comparisonAggregatedMetrics = comparandData.SelectMany(d => d.Value.Values).SelectMany(d => d.Values);
        table.AppendLine($"| {comparandConfigName} | {aggregation.Title} | " + string.Join(" | ", comparisonAggregatedMetrics.Select(d => d.GetValueOrDefault().ToString("N2"))) + " |");

        // Add comparison data.
        table.AppendLine($"| Comparison | % Diff | " + string.Join(" | ", baselineAggregatedMetrics.Zip(comparisonAggregatedMetrics, (b, c) => ((c - b) / b * 100).GetValueOrDefault().ToString("F2"))) + " |"); 
        table.AppendLine("\n");

        // Anomalies Section.
        List<(string key, int idx, double? anomolousVal, IEnumerable<double?> allVals, double volatility)> anomalies = new();
        foreach (var iteration in iterationData.Where(gc => GetBenchmarkFromKey(gc.Key) == benchmark.Key))
        {
            double volatilityThresholdToUse = volatilityThreshold;
            var overrideKey =  metrics.FirstOrDefault(m => m.Title == GetMetricFromKey(iteration.Key));
            if (overrideKey != null && (metricVolOverridePercentage.TryGetValue(overrideKey, out var overrideVol)))
            {
                volatilityThresholdToUse = overrideVol;
            }

            ClusteringResult clusterResult = Clustering.KMeansClusteringWithAnomalyDetection(iteration.Value.ToList());
            if (clusterResult.HasAnomaly)
            {
                // vol is the min-max % diff
                double? vol = (iteration.Value.Max() - iteration.Value.Min()) / iteration.Value.Min() * 100;

                // Go through anomalies.
                foreach (var anomaly in clusterResult.Anomalies)
                {
                    int idx = iteration.Value.ToList().IndexOf(anomaly);
                    anomalies.Add((iteration.Key, idx, anomaly, iteration.Value, vol.GetValueOrDefault()));
                }

                // Go through single element clusters.
                foreach (var cluster in clusterResult.Clusters)
                {
                    if (cluster.Count == 1)
                    {
                        int idx = iteration.Value.ToList().IndexOf(cluster.DataPoints.First());
                        anomalies.Add((iteration.Key, idx, cluster.DataPoints.First(), iteration.Value, vol.GetValueOrDefault()));
                    }
                }
            }
        }

        if (anomalies.Count > 0)
        {
            table.AppendLine($"### Anomalies");
            table.AppendLine();
            table.AppendLine($"| Config | Iteration | Metric | Vol % | Anomalous Value | {string.Join("|", Enumerable.Range(0, anomalies[0].allVals.Count()))} |");
            table.AppendLine("| ------ | --------- | ------ | ---------- | ----------- | " + string.Join(" | ", Enumerable.Range(0, anomalies[0].allVals.Count()).Select(i => "------")) + " |");
            foreach (var anomaly in anomalies.OrderByDescending(a => a.volatility))
            {
                string key = anomaly.key;
                string config = GetConfigFromKey(key);
                string metric = GetMetricFromKey(key);
                int iteration = anomaly.idx;
                table.AppendLine($"| {config} | {anomaly.idx} | {metric}  | {Math.Round(anomaly.volatility, 2)} | {Math.Round(anomaly.anomolousVal.GetValueOrDefault(), 2)} | {string.Join(" | ", anomaly.allVals.Select(a => a.GetValueOrDefault().ToString("N2")))} | ");
            }

            allAnomalies.AddRange(anomalies);
        }
    }

    StringBuilder sb = new();
    sb.AppendLine($"## Anomalies in Iterations"); 
    foreach (var volatilityVals in allAnomalies.OrderByDescending(r => r.volatility).Take(iterationsToDisplay))
    {
        sb.AppendLine($"   - __{GetConfigFromKey(volatilityVals.key)}-{GetBenchmarkFromKey(volatilityVals.key)}__ for {GetMetricFromKey(volatilityVals.key)} has an anomaly {Math.Round(volatilityVals.anomolousVal.GetValueOrDefault(), 2)} for the {volatilityVals.idx} iteration with Volatility: {Math.Round(volatilityVals.volatility, 2)}%:  {string.Join(", ", volatilityVals.allVals.Select(v => Math.Round(v.GetValueOrDefault(), 2)))}.");
    }
    
    sb.ToString().DisplayAs("text/markdown");

    if (display)
    {
        table.ToString().DisplayAs("text/markdown");
    }
}

Dictionary<string, Dictionary<string, Dictionary<string, Dictionary<int, double?>>>> GetIterationData_Raw(DataManager dataManager, List<Metric<IterationData>> metrics,
    Filter runFilter = null, Filter configFilter = null, Filter benchmarkFilter = null, IntFilter iterationFilter = null,
    ConfigIterationFilter configIterationFilter = null, Func<IterationData, bool> dataFilter = null,
    Func<string, string> benchmarkMap = null, XArrangement xArrangement = null,
    NameSimplifier configNameSimplifier = null, bool includeRunName = false,
    bool display = true, bool debug = false)
{
    IterationsChartType chartType = new IterationsChartType();
    var xMetric = chartType.DefaultXMetric; 
    var result = GetDataInternal(chartType: chartType,
        dataManager: dataManager, metrics: metrics,
        runFilter: runFilter, benchmarkFilter: benchmarkFilter, iterationFilter: iterationFilter, configFilter: configFilter,
        configIterationFilter: configIterationFilter, dataFilter: dataFilter,
        xArrangement: xArrangement,
        configNameSimplifier: configNameSimplifier, includeRunName: includeRunName, debug: debug);

    Dictionary<string, Dictionary<string, Dictionary<string, Dictionary<int, double?>>>> data = new();

    //foreach (var r in result)
    for (int i = 0; i < result.Count; i++)
    {
        var r = result[i];

        List<(XValue, double?)> metricData = null;
        try { metricData = r.Item2.Select(b => (x: xMetric.DoExtract((b.Key, b.Value)), y: r.Item1.Metric.DoExtract(b.Value, i))).ToList(); }
        catch { Console.WriteLine($"Exception processing data items"); metricData = null; } 

        if (metricData != null)
        {
            // Config
            if (!data.TryGetValue(r.Item1.Config, out var configData))
            {
                data[r.Item1.Config] = configData = new();
            }

            // Metric
            if (!configData.TryGetValue(r.Item1.Metric.Title, out var m))
            {
                configData[r.Item1.Metric.Title] = m = new();
            }

            // Benchmark 
            foreach (var metric in metricData)
            {
                if (!m.TryGetValue(metric.Item1.GetName(), out var itData))
                {
                    m[metric.Item1.GetName()] = itData = new(); 
                }

                itData[r.Item1.Iteration.GetValueOrDefault()] = metric.Item2;
            }
        }

        else
        {
            // TODO: Log.
        }
    }

    return data; 
}

#!csharp

double CalculateMean(List<double> data, int endIndex = -1)
{
    if (endIndex == -1)
        endIndex = data.Count - 1;

    double sum = 0;
    for (int i = 0; i <= endIndex; i++)
    {
        sum += data[i];
    }

    return sum / (endIndex + 1);
}

double CalculateVariance(List<double> data, double mean, int endIndex = -1)
{
    if (endIndex == -1)
        endIndex = data.Count - 1;

    double sum = 0;
    for (int i = 0; i <= endIndex; i++)
    {
        sum += Math.Pow(data[i] - mean, 2);
    }

    return sum / (endIndex + 1);
}

List<(int start, int end)> FindStabilizationPoint2(List<double> gcs, double threshold)
{
    var data = gcs.Where(gc => !double.IsNaN(gc) && gc != 0).ToList();
    List<(int start, int end)> stabilizationPoints = new List<(int start, int end)>();
    double mean = CalculateMean(data);
    double variance = CalculateVariance(data, mean);

    int stabilizationPoint = -1;
    for (int i = 0; i < data.Count; i++)
    {
        double currentMean = CalculateMean(data, i);
        double currentVariance = CalculateVariance(data, currentMean, i);

        if (Math.Abs(currentMean - mean) <= threshold && Math.Abs(currentVariance - variance) <= threshold)
        {
            stabilizationPoint = i;
            break;
        }
    }

    if (stabilizationPoint != -1)
    {
        // Split sections based on level
        double level = data.Skip(stabilizationPoint).Average();
        List<double> currentSection = new List<double>();
        int sectionStart = stabilizationPoint;

        for (int i = stabilizationPoint; i < data.Count; i++)
        {
            if (Math.Abs(data[i] - level) <= threshold)
            {
                currentSection.Add(data[i]);
            }
            else
            {
                if (currentSection.Count > 0)
                {
                    stabilizationPoints.Add((sectionStart, i - 1));
                    currentSection.Clear();
                }
                level = data[i];
                sectionStart = i;
                currentSection.Add(data[i]);
            }
        }

        if (currentSection.Count > 0)
        {
            stabilizationPoints.Add((sectionStart, data.Count - 1));
        }
    }

    return stabilizationPoints;
}

int FindStabilizationPoint(List<double> gcs, double threshold)
{
    var data = gcs.Where(gc => !double.IsNaN(gc) && gc != 0);
    int stabilizationPoint = -1;
    double mean = CalculateMean(data);
    double variance = CalculateVariance(data, mean);

    for (int i = 0; i < data.Count; i++)
    {
        double currentMean = CalculateMean(data, i);
        double currentVariance = CalculateVariance(data, currentMean, i);

        // Try: with just variance.
        if (Math.Abs(currentMean - mean) <= threshold && Math.Abs(currentVariance - variance) <= threshold)
        {
            stabilizationPoint = i;
            break;
        }
    }

    return stabilizationPoint;
}

string GetBenchmarkFromKey(string key) => key.Split('|')[1];
string GetConfigFromKey(string key) => key.Split('|')[0];
string GetIterationFromKey(string key) => key.Split('|')[2];
string CreateKey(string configName, string benchmarkName, int? iteration) => $"{configName}|{benchmarkName}|{iteration}";

public record TCPData(double avg, double sum, long count, List<double> vals, double stableAverage, long pointOfStabilization, long gcIndex);

public Dictionary<string, TCPData> GetPerIterationTCPData(DataManager dm)
{
    var allTraceGCData = GetDataInternal<TraceGC>(dataManager: dm, chartType: new TraceGCChartType(), metrics: ML( Metrics.G.TCPToConsider ));
    Dictionary<string, TCPData> perIterationData = new();
    foreach (var pd in allTraceGCData)
    {
        string key = CreateKey(pd.Item1.Config, pd.Item1.Benchmark, pd.Item1.Iteration);

        double sum = 0;
        long count = 0;
        List<double> vals = new();

        foreach (var g in pd.Item2)
        {
            var gc = g.Value;
            var tcpToConsider = gc.DynamicEvents().SizeAdaptationTuning?.MedianThroughputCostPercent;
            if (tcpToConsider != null && tcpToConsider > 0)
            {
                ++count;
                sum += tcpToConsider;
                vals.Add(tcpToConsider);
            }
        }

        double avg = sum / count;
        double stableAverage = avg;
        long gcIndex = -1;
        int pointOfStabilization = FindStabilizationPoint(vals, 0.1);
        if (pointOfStabilization != -1)
        {
            stableAverage = vals.Skip(pointOfStabilization).Average();
            gcIndex = pd.Item2[pointOfStabilization].Value.Number;
        }


        perIterationData[key] = new TCPData(avg, sum, count, vals, stableAverage, pointOfStabilization, gcIndex); 
    }

    return perIterationData;
}

public class TCPDataBenchmarkDiff
{
    public TCPData baseline {get; set; } 
    public TCPData comparand {get; set;} 
    public double percentDifference {get;set;}
    public double stablePercentDifference {get;set;}
    public override string ToString() => $"Baseline: {Math.Round(baseline.avg, 2)}, Comparand: {Math.Round(comparand.avg, 2)}, % Diff: {Math.Round(percentDifference, 2)}";
}

public Dictionary<string, TCPDataBenchmarkDiff> GetPerBenchmarkTCPData(DataManager dm, string baselineConfig, string comparandConfig)
{
    Dictionary<string, TCPDataBenchmarkDiff> perBenchmarkData = new();
    var perIterationData = GetPerIterationTCPData(dm);

    // Aggregate per benchmark.
    foreach (var pd in perIterationData)
    {
        string config = GetConfigFromKey(pd.Key);
        if (config != baselineConfig && config != comparandConfig)
        {
            continue;
        }

        string benchmark = GetBenchmarkFromKey(pd.Key);
        if (!perBenchmarkData.TryGetValue(benchmark, out var tcpData))
        {
            perBenchmarkData[benchmark] = tcpData = new TCPDataBenchmarkDiff();
        }
        
        if (config == baselineConfig)
        {
            tcpData.baseline = pd.Value;
        }
        else
        {
            tcpData.comparand = pd.Value;
        }
    }

    foreach (var b in perBenchmarkData)
    {
        b.Value.percentDifference = (b.Value.comparand.avg - b.Value.baseline.avg) / b.Value.baseline.avg * 100;
        b.Value.stablePercentDifference = (b.Value.comparand.stableAverage - b.Value.baseline.stableAverage ) / b.Value.baseline.stableAverage * 100;
    }

    return perBenchmarkData;
}

#!csharp

public void SummarizeBenchmarkTCPData(DataManager dm, string baselineConfig, string comparandConfig, int numberOfBenchmarksToDisplay = 5, bool display = true)
{
    var perBenchmarkData = GetPerBenchmarkTCPData(dm, baselineConfig, comparandConfig);
    StringBuilder sb = new();
    sb.AppendLine($"## TCP Data Summary Per Benchmark");
    foreach (var b in perBenchmarkData.OrderByDescending(p => p.Value.percentDifference).Take(numberOfBenchmarksToDisplay))
    {
        sb.AppendLine($"   - __{b.Key}__: {Math.Round(b.Value.percentDifference, 2)}% with Baseline: {Math.Round(b.Value.baseline.avg, 2)}% and Comparand: {Math.Round(b.Value.comparand.avg, 2)}%");
    }

    sb.ToString().DisplayAs("text/markdown");

    if (display)
    {
        sb.AppendLine($"| Benchmark | {baselineConfig} | {comparandConfig} | % Difference |");
        sb.AppendLine("| --------- | ------------- | ------------ | ------------ |");

        int  i=  0;
        foreach (var d in perBenchmarkData.OrderByDescending(p => p.Value.percentDifference)) 
        {
            if (double.IsNaN(d.Value.percentDifference))
            {
                continue;
            }

            sb.AppendLine($"| {i++}. {d.Key} | {Math.Round(d.Value.baseline.stableAverage, 2)} | {Math.Round(d.Value.comparand.stableAverage, 2)} | {Math.Round(d.Value.stablePercentDifference, 2)} |");
        }
    }
}
